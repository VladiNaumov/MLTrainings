# -*- coding: utf-8 -*-
"""04. Классификация, логистическая регрессия

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J6VD7MwujvbdQqBdlc3ElFz-QA0DF3EK

# 04. Классификация, логистическая регрессия
"""

import ipywidgets as widgets
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt # для визуализации
import seaborn as sns
sns.set_style("darkgrid", {"grid.color": ".6", "grid.linestyle": ":"})
sns.set(rc={'figure.figsize':(11.7,8.27)})

import os # для обработки пути к данным как на Unix-системах, так и на Windows

"""## Data"""

iris = sns.load_dataset("iris")

iris

sns.pairplot(iris, hue='species')

"""## Activation functions"""

def sigmoid(x):  
    return np.exp(-np.logaddexp(0, -x))

sigmoid(-100)

X_fake = np.linspace(-10, 10, 1000)
plt.plot(X_fake, [sigmoid(y) for y in X_fake])

"""## Log Loss

$$ L(y, \hat{y}) = - \frac{1}{m} \sum_{i = 0}^{m}{(y_i\log{\hat{y_i}} + (1 - y) \log(1 - \hat{y}))} $$
"""

def log_loss_score(predicted, actual, eps=1e-14):
  """The logarithmic loss between between the predicted probability assigned to the possible outcomes for item i, and the actual outcome."""

  predicted = np.clip(predicted, eps, 1-eps)
  loss = -1 * np.mean(actual * np.log(predicted) + (1 - actual) * np.log(1-predicted))
  return loss

log_loss_score(0.5, 1)

"""## Softmax

$$ y = \frac{e^{y_i}}{\sum_{i = 0}^{num\_of\_classes}{e^{y_i}}} $$
"""

def softmax(x):
  """Compute softmax values for each sets of scores in x."""
  return np.exp(x) / np.sum(np.exp(x), axis=0)

logits = [2.0, 1.0, 0.1]
softmax(logits)

"""## Benefits of vectorization"""

N = 10000

weights = np.random.rand(N) # generate N weights
X = np.random.randint(0, 100, N) # generate N features

X

def unvectorized(weights, X):
    prediction = 0.0
    for weight, x in zip(weights, X):
        prediction += weights * x
    return prediction

def vectorized(weights, X):
    prediction = np.dot(weights.T , X)
    return prediction

"""**Preformanse comparison:**"""

# Commented out IPython magic to ensure Python compatibility.
# %timeit -n100 unvectorized(weights, X)

# Commented out IPython magic to ensure Python compatibility.
# %timeit -n100 vectorized(weights, X)

"""## sklearn"""

X = iris.drop(['species'], axis=1)

X

y = iris['species']

np.unique(y)

"""## sklearn"""

from sklearn.linear_model import LogisticRegression
from sklearn import datasets

clf = LogisticRegression()

clf.fit(X, y)

"""## Model evaluation"""

clf.predict([[3, 3, 3, 3]])

print(f'Score: {clf.score(X, y)}')

# import some data to play with
iris = datasets.load_iris()
X = iris.data[:, :2]  # we only take the first two features.
Y = iris.target

# Create an instance of Logistic Regression Classifier and fit the data.
logreg = LogisticRegression(C=1e5)
logreg.fit(X, Y)

# Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, x_max]x[y_min, y_max].
x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
h = .02  # step size in the mesh
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])

# Put the result into a color plot
Z = Z.reshape(xx.shape)
plt.figure(1, figsize=(10, 8))
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)

# Plot also the training points
plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')

plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.xticks(())
plt.yticks(())

plt.show()

from sklearn.neighbors import KNeighborsClassifier

# import some data to play with
iris = datasets.load_iris()
X = iris.data[:, :2]  # we only take the first two features.
Y = iris.target

# Create an instance of Logistic Regression Classifier and fit the data.
logreg = KNeighborsClassifier(n_neighbors=11)
logreg.fit(X, Y)

# Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, x_max]x[y_min, y_max].
x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
h = .02  # step size in the mesh
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])

# Put the result into a color plot
Z = Z.reshape(xx.shape)
plt.figure(1, figsize=(10, 8))
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)

# Plot also the training points
plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')

plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.xticks(())
plt.yticks(())

plt.show()

